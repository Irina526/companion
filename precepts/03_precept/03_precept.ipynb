{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "djrwdflkeaS2"
   },
   "source": [
    "# 03 Precept: Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FbGmeQRPetlj"
   },
   "source": [
    "Agenda: \n",
    "- Lecture Review\n",
    "- Coding Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dzh0sUHbf2kU"
   },
   "source": [
    "## Lecture Review - Least Squares\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q_OomSw2g0jv"
   },
   "source": [
    "## Topics covered\n",
    "\n",
    " - Cholesky factorization\n",
    "\n",
    "### Least Squares\n",
    "- Least squares optimization\n",
    "*  Gram matrix\n",
    "-  Solving least squares\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zf7B_1tyIIc"
   },
   "source": [
    "### Complexity of operations:\n",
    "\n",
    "In the table below, $A$ and $B$ are $m \\times n$ matrices, $C$ is an $m \\times p$ matrix, $x$ is an $n$ -vector, and $a$ is a scalar.\n",
    "\n",
    "Matrix operations  | Complexity (flops)\n",
    "-------------------|------------------\n",
    "$aA $     | $mn $\n",
    "$A+B $|$mn $\n",
    "$Ax$ | $2mn $\n",
    " $AC$ | $2mnp $\n",
    " $A^TA$| $mn^2$\n",
    "\n",
    "\n",
    "\n",
    "### Factorization\n",
    " Matrix operations      | Complexity (flops)\n",
    "------------------------|---------------------------------------------------\n",
    " $LU$ - Factorization   | $(2/3)n^3+2 n^{2} \\approx (2 / 3) n^{3}$\n",
    " $LL^T$ - Factorization | $(1 / 3) n^{3}+2 n^{2} \\approx (1 / 3) n^{3}$\n",
    "\n",
    "### Solving normal equations of least squares problems:\n",
    "\n",
    "Methods                                | Complexity (flops)\n",
    "---------------------------------------|------------------\n",
    "Inversion                              |  $O(n^3)$\n",
    "Solving normal equations with Cholesky |$2 m n^{2}+2 m n+(1 / 3) n^{3}+2 n^{2} \\approx 2 m n^{2}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4HtjlIuY3dN6"
   },
   "source": [
    "### Gram matrix and its properties \n",
    "  - Invertibility \n",
    "  - Positive (semi)definiteness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OKj0p9wAvdTs"
   },
   "source": [
    "#### Clarification\n",
    "\n",
    "- Positive (semi)definiteness v.s. Matrix comparison relationships \n",
    "    \n",
    "    - Definition: Positive (semi)definiteness\n",
    "    \n",
    "    - On the other hand, for any two matrices  $M, Q \\in \\mathbb{R}^{N \\times N}$ we write $M \\gg Q$ if $m_{j k}>q_{j k}$, $M \\succ Q$ if $m_{j k} \\geq q_{j k},$ for any $j, k$, but $M \\neq Q$, and $M \\succeq Q$ if $m_{j k} \\geq q_{j k}$ for any $j, k$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparsity of factorization vs inverse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 1.],\n",
       "       [0., 1., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 1.],\n",
       "       [0., 0., 0., 1., 1.],\n",
       "       [1., 1., 1., 1., 1.]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "from scipy.linalg import lu\n",
    "\n",
    "n = 5\n",
    "A = np.eye(n)\n",
    "A[:, -1] = np.ones(n)\n",
    "A[-1, :] = np.ones(n)\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L = \n",
      " [[1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "U = \n",
      " [[ 1.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  1.]\n",
      " [ 0.  0.  1.  0.  1.]\n",
      " [ 0.  0.  0.  1.  1.]\n",
      " [ 0.  0.  0.  0. -3.]]\n"
     ]
    }
   ],
   "source": [
    "P, L, U = lu(A)\n",
    "print(\"L = \\n\", L)\n",
    "print(\"U = \\n\", U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A_inv = \n",
      " [[ 0.66666667 -0.33333333 -0.33333333 -0.33333333  0.33333333]\n",
      " [-0.33333333  0.66666667 -0.33333333 -0.33333333  0.33333333]\n",
      " [-0.33333333 -0.33333333  0.66666667 -0.33333333  0.33333333]\n",
      " [-0.33333333 -0.33333333 -0.33333333  0.66666667  0.33333333]\n",
      " [ 0.33333333  0.33333333  0.33333333  0.33333333 -0.33333333]]\n"
     ]
    }
   ],
   "source": [
    "A_inv = inv(A)\n",
    "print(\"A_inv = \\n\", A_inv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tOo7Op6T4Q2b"
   },
   "source": [
    "## Least squares orthogonality principle\n",
    "\n",
    "The point $A x^\\star$ is the linear combination of the columns of $A$ that is closest to $b .$ The optimal residual is $r=A x^\\star -b .$ The optimal residual satisfies a property that is sometimes called the orthogonality principle:\n",
    "It is orthogonal to the columns of $A,$ and therefore, it is orthogonal to any linear combination of the columns of $A$. In other words, for any $n$-vector $z$, we have\n",
    "$$\n",
    "(A z) \\perp r \\quad \\iff \\quad (Ax)^Tr = 0\n",
    "$$\n",
    "We can derive the orthogonality principle from the normal equations, which can be expressed as $A^{T}(A x^\\star -b)=0$. For any $n$-vector $z$ we have\n",
    "$$\n",
    "(A z)^{T} r=(A z)^{T}(A x^\\star -b)=z^{T} A^{T}(A x^\\star - b) = 0\n",
    "$$\n",
    "\n",
    "![orthogonality](orthogonality.png)\n",
    "\n",
    "The orthogonality principle is illustrated in the figure above, for a least squares problem with $m=3$ and $n=2$. The shaded plane is the set of all linear combinations $z_{1} a_{1}+z_{2} a_{2}$ of $a_{1}$ and $a_{2}$, the two columns of $A$. The point $A x^\\star$ is the closest point in the plane to $b$. The optimal residual $r$ is shown as the vector from $b$ to $A x^\\star$. This vector is orthogonal to any point in the shaded plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIMzqF0I5eDD"
   },
   "source": [
    "## Solving least-squares problems in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages for coding exercises\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy.linalg import cholesky as llt\n",
    "seed = 1\n",
    "np.random.seed(seed) # We use np.random.seed for reproducible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience we have included the following function `lstsq` that solves the least squares problem as we have seen in class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_substitution(L, b):\n",
    "    n = L.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        x[i] = (b[i] - L[i,:i] @ x[:i])/L[i, i]\n",
    "    return x\n",
    "\n",
    "def backward_substitution(U, b):\n",
    "    n = U.shape[0]\n",
    "    x = np.zeros(n)\n",
    "    for i in reversed(range(n)):\n",
    "        x[i] = (b[i] - U[i,i+1:] @ x[i+1:])/U[i, i]\n",
    "    return x\n",
    "\n",
    "def lstsq(A, b):\n",
    "    M = A.T.dot(A)  # Form Gram matrix\n",
    "    q = A.T.dot(b)  # Form right hand side\n",
    "    L = llt(M)      # Factor\n",
    "    x = forward_substitution(L, q)\n",
    "    x = backward_substitution(L.T, x)\n",
    "    return x, L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a random $20\\times 10$ matrix $A$ and a random $20$-vector $b$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.rand(20, 10)\n",
    "b = np.random.rand(20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(a) Compare the solution $x^{\\star}$ of the associated least-squares problem using pseudoinverse `np.linalg.pinv` and the method in class `lstsq` from above. Verify that the solutions are the same, or more accurately, very close to each other. They might be slightly different due to small roundoff errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(b) Let $x^{\\star}$ be one of the solutions found in part (a). Now generate a random nonzero\n",
    "$10$-vector $\\delta$ and show that $\\|A(x^{\\star} +\\delta)-b\\|_2^2> \\|Ax^{\\star} -b\\|_2^2$. Repeat several times with different values of $\\delta$, you might try choosing a small $\\theta$ by scaling the random vector obtained with `np.random.randn`"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "03_precept.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
